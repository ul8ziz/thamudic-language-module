"""
ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø«Ù…ÙˆØ¯ÙŠØ©
"""

import os
<<<<<<< Updated upstream
from PIL import Image, ImageEnhance
from core.inference_engine import InferenceEngine

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒØªØ§Ø¨Ø§Øª Ø§Ù„Ø«Ù…ÙˆØ¯ÙŠØ©",
    page_icon="ğŸ”",
    layout="wide"
)

# CSS Ù„Ù„ØªØµÙ…ÙŠÙ… Ù…Ù† Ø§Ù„ÙŠÙ…ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø±
st.markdown("""
<style>
    .element-container, .stMarkdown, .stButton, .stText {
        direction: rtl;
        text-align: right;
    }
    .st-emotion-cache-16idsys p {
        direction: rtl;
        text-align: right;
    }
    .stMetricValue, .stMetricLabel {
        direction: rtl;
        text-align: right;
    }
</style>
""", unsafe_allow_html=True)

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
MODEL_PATH = os.path.join('models', 'best_model.pth')
LABEL_MAPPING_PATH = os.path.join('models', 'configs', 'label_mapping.json')

@st.cache_resource
def load_predictor():
    return InferenceEngine(MODEL_PATH, LABEL_MAPPING_PATH)

def preprocess_image(image, contrast=2.0, sharpness=1.5, brightness=1.2, auto_rotate=True):
    """ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ø¹ Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"""
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ RGB Ø¥Ø°Ø§ ÙƒØ§Ù†Øª RGBA
    if image.mode == 'RGBA':
        image = image.convert('RGB')
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ ØªØ¯Ø±Ø¬Ø§Øª Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠ
    image = image.convert('L')
    
    # ØªØµØ­ÙŠØ­ Ø¯ÙˆØ±Ø§Ù† Ø§Ù„ØµÙˆØ±Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø·Ù„ÙˆØ¨Ø§Ù‹
    if auto_rotate:
        try:
            from PIL import ExifTags
            try:
                for orientation in ExifTags.TAGS.keys():
                    if ExifTags.TAGS[orientation] == 'Orientation':
                        break
                exif = dict(image._getexif().items())
                if exif[orientation] == 3:
                    image = image.rotate(180, expand=True)
                elif exif[orientation] == 6:
                    image = image.rotate(270, expand=True)
                elif exif[orientation] == 8:
                    image = image.rotate(90, expand=True)
            except (AttributeError, KeyError, IndexError):
                # Ø§Ù„ØµÙˆØ±Ø© Ù„Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª EXIF
                pass
        except:
            pass
    
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ø·ÙˆØ¹
    enhancer = ImageEnhance.Brightness(image)
    image = enhancer.enhance(brightness)
    
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¨Ø§ÙŠÙ†
    enhancer = ImageEnhance.Contrast(image)
    image = enhancer.enhance(contrast)
    
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø­Ø¯Ø©
    enhancer = ImageEnhance.Sharpness(image)
    image = enhancer.enhance(sharpness)
    
    return image

def main():
    st.title("Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒØªØ§Ø¨Ø§Øª Ø§Ù„Ø«Ù…ÙˆØ¯ÙŠØ© ğŸ”")
    st.markdown("---")

    try:
        predictor = load_predictor()
        st.success("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")

        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©
        with st.expander("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©"):
            col1, col2, col3 = st.columns(3)
            with col1:
                contrast = st.slider("Ø§Ù„ØªØ¨Ø§ÙŠÙ†", 0.5, 3.0, 2.0, 0.1)
            with col2:
                sharpness = st.slider("Ø§Ù„Ø­Ø¯Ø©", 0.5, 3.0, 1.5, 0.1)
            with col3:
                brightness = st.slider("Ø§Ù„Ø³Ø·ÙˆØ¹", 0.5, 2.0, 1.2, 0.1)
            auto_rotate = st.checkbox("ØªØµØ­ÙŠØ­ Ø¯ÙˆØ±Ø§Ù† Ø§Ù„ØµÙˆØ±Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹", value=True)

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©
        uploaded_file = st.file_uploader(
            "Ø§Ø®ØªØ± ØµÙˆØ±Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„",
            type=['png', 'jpg', 'jpeg']
        )

        if uploaded_file:
            # Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
            col2, col1 = st.columns(2)
            
            with col1:
                st.subheader("Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„")
                with st.spinner('Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©...'):
                    try:
                        # Ù‚Ø±Ø§Ø¡Ø© ÙˆØ¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
                        original_image = Image.open(uploaded_file)
                        
                        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©
                        processed_image = preprocess_image(
                            original_image,
                            contrast=contrast,
                            sharpness=sharpness,
                            brightness=brightness,
                            auto_rotate=auto_rotate
                        )
                        
                        # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
                        temp_path = "temp_image.jpg"
                        processed_image.save(temp_path)
                        
                        # Ø§Ù„ØªÙ†Ø¨Ø¤
                        results = predictor.predict(temp_path)
                        
                        if results:
                            st.write(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results)} Ø­Ø±Ù")
                            
                            # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØ±Ø¯ÙŠØ©
                            for i, result in enumerate(results):
                                letter = result['letter']
                                thamudic_symbol = predictor.thamudic_symbols.get(letter, '?')
                                confidence = result['confidence']
                                
                                # ØªÙ„ÙˆÙŠÙ† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø­Ø³Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©
                                if confidence >= 0.8:
                                    confidence_color = "ğŸŸ¢"  # Ø£Ø®Ø¶Ø± Ù„Ù„Ø«Ù‚Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©
                                elif confidence >= 0.5:
                                    confidence_color = "ğŸŸ¡"  # Ø£ØµÙØ± Ù„Ù„Ø«Ù‚Ø© Ø§Ù„Ù…ØªÙˆØ³Ø·Ø©
                                else:
                                    confidence_color = "ğŸ”´"  # Ø£Ø­Ù…Ø± Ù„Ù„Ø«Ù‚Ø© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø©
                                
                                st.metric(
                                    f"Ø§Ù„ØªÙ†Ø¨Ø¤ #{i+1} {confidence_color}",
                                    f"{letter} ({thamudic_symbol})",
                                    f"Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©: {confidence:.2%}"
                                )
                            
                            # Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„
                            full_text = "".join([result['letter'] for result in results])
                            full_text_thamudic = "".join([predictor.thamudic_symbols.get(result['letter'], '?') for result in results])
                            
                            st.markdown("### Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªØ¹Ø±Ù Ø¹Ù„ÙŠÙ‡")
                            st.markdown(f"""
                            <div style='direction: rtl; text-align: right; font-size: 24px; 
                                        padding: 20px; background-color: #1e3d59; border-radius: 10px;
                                        color: #ffc13b; font-weight: bold; margin: 10px 0;
                                        border: 2px solid #ffc13b;'>
                                <div style='margin-bottom: 10px;'>Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ: {full_text}</div>
                                <div>Ø§Ù„Ù†Øµ Ø§Ù„Ø«Ù…ÙˆØ¯ÙŠ: {full_text_thamudic}</div>
                            </div>
                            """, unsafe_allow_html=True)
                        else:
                            st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù†ØªØ§Ø¦Ø¬")
                            
                    except Exception as pred_error:
                        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤: {str(pred_error)}")
                        st.write("Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©:")
                        st.write(f"Ù†ÙˆØ¹ Ø§Ù„Ø®Ø·Ø£: {type(pred_error).__name__}")
                        import traceback
                        st.code(traceback.format_exc())

            with col2:
                # Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±
                st.subheader("Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©")
                st.image(original_image, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©")
                
                st.subheader("Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
                st.image(processed_image, caption="Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")

            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
            if os.path.exists(temp_path):
                os.remove(temp_path)

    except Exception as e:
        st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")
        st.write("Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©:")
        st.write(f"Ù†ÙˆØ¹ Ø§Ù„Ø®Ø·Ø£: {type(e).__name__}")
        import traceback
        st.code(traceback.format_exc())
=======
import torch
import cv2
import numpy as np
from pathlib import Path
import json
import logging
from PIL import Image
import streamlit as st
from thamudic_model import ThamudicRecognitionModel

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)

class ThamudicRecognitionApp:
    """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø«Ù…ÙˆØ¯ÙŠØ©"""
    
    def __init__(self, model_path: str, mapping_file: str, image_size: tuple = (224, 224)):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
        
        Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:
            model_path: Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            mapping_file: Ù…Ø³Ø§Ø± Ù…Ù„Ù ØªØ¹ÙŠÙŠÙ† Ø§Ù„ÙØ¦Ø§Øª
            image_size: Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.image_size = image_size
        
        # ØªØ­Ù…ÙŠÙ„ ØªØ¹ÙŠÙŠÙ† Ø§Ù„ÙØ¦Ø§Øª
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
            self.class_mapping = {item['name']: item['index'] for item in mapping_data['thamudic_letters']}
            
        # Ø¥Ù†Ø´Ø§Ø¡ ØªØ¹ÙŠÙŠÙ† Ø¹ÙƒØ³ÙŠ (Ù…Ù† Ø§Ù„ÙÙ‡Ø±Ø³ Ø¥Ù„Ù‰ Ø§Ù„ÙØ¦Ø©)
        self.idx_to_class = {idx: name for name, idx in self.class_mapping.items()}
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        self.model = ThamudicRecognitionModel(num_classes=len(self.class_mapping))
        self.load_model(model_path)
        self.model.eval()
    
    def load_model(self, model_path: str):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ù…Ù„Ù"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model = self.model.to(self.device)
            logging.info(f"Loaded model from {model_path}")
        except Exception as e:
            logging.error(f"Error loading model: {str(e)}")
            raise
    
    def preprocess_image(self, image: np.ndarray) -> torch.Tensor:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© Ù‚Ø¨Ù„ Ø§Ù„ØªØ¹Ø±Ù
        
        Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:
            image: Ù…ØµÙÙˆÙØ© numpy Ù„Ù„ØµÙˆØ±Ø©
            
        Returns:
            tensor: ØªÙ†Ø³ÙˆØ± PyTorch Ø¬Ø§Ù‡Ø² Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
        """
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªØ¯Ø±Ø¬ Ø±Ù…Ø§Ø¯ÙŠ
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image
            
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¨Ø§ÙŠÙ†
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        enhanced = clahe.apply(gray)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø¹ØªØ¨Ø© ØªÙƒÙŠÙÙŠØ©
        binary = cv2.adaptiveThreshold(
            enhanced, 255, 
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY_INV, 11, 2
        )
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡
        kernel = np.ones((3,3), np.uint8)
        cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, kernel)
        
        # ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù…
        resized = cv2.resize(cleaned, self.image_size)
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙˆØ±
        tensor = torch.from_numpy(resized).float()
        tensor = tensor.unsqueeze(0).unsqueeze(0)  # Ø¥Ø¶Ø§ÙØ© Ø¨ÙØ¹Ø¯ Ø§Ù„Ø¯ÙØ¹Ø© ÙˆØ§Ù„Ù‚Ù†Ø§Ø©
        tensor = tensor / 255.0  # ØªØ·Ø¨ÙŠØ¹
        
        return tensor
    
    def predict(self, image: np.ndarray, top_k: int = 3) -> list:
        """
        Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±Ù ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©
        
        Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:
            image: Ù…ØµÙÙˆÙØ© numpy Ù„Ù„ØµÙˆØ±Ø©
            top_k: Ø¹Ø¯Ø¯ Ø£ÙØ¶Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            
        Returns:
            list: Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø£ÙØ¶Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ø¹ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø«Ù‚Ø©
        """
        try:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©
            tensor = self.preprocess_image(image)
            tensor = tensor.to(self.device)
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤
            with torch.no_grad():
                outputs = self.model(tensor)
                probabilities = torch.nn.functional.softmax(outputs, dim=1)
                
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
            top_probs, top_indices = torch.topk(probabilities, k=min(top_k, len(self.class_mapping)))
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø©
            predictions = []
            for prob, idx in zip(top_probs[0], top_indices[0]):
                idx_val = idx.item()
                class_name = self.idx_to_class[idx_val]
                confidence = prob.item() * 100
                predictions.append({
                    'class': class_name,
                    'confidence': confidence
                })
            
            return predictions
            
        except Exception as e:
            logging.error(f"Error during prediction: {str(e)}")
            raise

def main():
    """Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚"""
    st.title("Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø«Ù…ÙˆØ¯ÙŠØ©")
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
    base_dir = Path(__file__).parent.parent
    model_path = base_dir / 'models' / 'thamudic_model.pth'
    mapping_file = base_dir / 'data' / 'mapping.json'
    
    if not model_path.exists():
        st.error("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙˆÙ„Ø§Ù‹.")
        return
        
    try:
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
        app = ThamudicRecognitionApp(
            model_path=str(model_path),
            mapping_file=str(mapping_file)
        )
        
        # ÙˆØ§Ø¬Ù‡Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±
        st.subheader("ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø©")
        uploaded_file = st.file_uploader(
            "Ø§Ø®ØªØ± ØµÙˆØ±Ø© Ù„Ù„Ø­Ø±Ù Ø§Ù„Ø«Ù…ÙˆØ¯ÙŠ",
            type=['png', 'jpg', 'jpeg']
        )
        
        if uploaded_file is not None:
            # Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø­Ù…Ù„Ø©
            image = Image.open(uploaded_file)
            st.image(image, caption='Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø­Ù…Ù„Ø©', use_column_width=True)
            
            # Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±Ù
            predictions = app.predict(np.array(image))
            
            # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            st.subheader("Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
            for pred in predictions:
                st.write(f"{pred['class']}: {pred['confidence']:.2f}%")
                
    except Exception as e:
        st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")
        logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚: {str(e)}")
>>>>>>> Stashed changes

if __name__ == '__main__':
    main()
