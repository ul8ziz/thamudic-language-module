"""
Thamudic Character Recognition Web Application
ØªØ·Ø¨ÙŠÙ‚ ÙˆÙŠØ¨ Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø«Ù…ÙˆØ¯ÙŠØ©
Streamlit-based web interface for Thamudic character recognition
ÙˆØ§Ø¬Ù‡Ø© ÙˆÙŠØ¨ Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Streamlit Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø«Ù…ÙˆØ¯ÙŠØ©
"""

import os
import logging
import json
import torch
from PIL import Image, ImageEnhance, ImageDraw, ImageFont
import streamlit as st
from models import ThamudicRecognitionModel
from torchvision import transforms
from pathlib import Path
import numpy as np
import cv2
import io

# Setup logging | Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)

class ThamudicApp:
    """
    Thamudic Character Recognition Application
    ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø«Ù…ÙˆØ¯ÙŠØ©
    """
    
    def __init__(self, model_path: str, mapping_path: str):
        """
        Initialize the application
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
        """
        # Set device | ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Ø²
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Load letter mapping | ØªØ­Ù…ÙŠÙ„ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø­Ø±ÙˆÙ
        with open(mapping_path, 'r', encoding='utf-8') as f:
            self.mapping_data = json.load(f)
            self.num_classes = len(self.mapping_data['thamudic_letters'])
            self.letter_mapping = {
                item['index']: {'letter': item['letter'], 'symbol': item['symbol']}
                for item in self.mapping_data['thamudic_letters']
            }
        
        # Initialize the model | ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        self.model = ThamudicRecognitionModel(num_classes=len(self.mapping_data['thamudic_letters']))
        self.load_model(model_path)
        self.model.eval()
        
    def load_model(self, model_path: str):
        """
        Load the trained Thamudic recognition model
        ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø«Ù…ÙˆØ¯ÙŠØ© Ø§Ù„Ù…Ø¯Ø±Ø¨
        """
        try:
            checkpoint = self.model.load_checkpoint(model_path)
            self.model = self.model.to(self.device)
            logging.info(f"Model loaded successfully from {model_path}")
        except Exception as e:
            logging.error(f"Error loading model: {str(e)}")
            raise

    def preprocess_image(self, image: Image.Image, 
                        contrast: float = 2.0,
                        brightness: float = 1.2,
                        sharpness: float = 1.5,
                        resize: bool = True,
                        rotate: int = 0) -> torch.Tensor:
        """
        Preprocess the uploaded image
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©
        
        Args:
            image: Uploaded image file | Ù…Ù„Ù Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙˆØ¹
            contrast: Contrast adjustment factor | Ù…Ø¹Ø§Ù…Ù„ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªØ¨Ø§ÙŠÙ†
            brightness: Brightness adjustment factor | Ù…Ø¹Ø§Ù…Ù„ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø³Ø·ÙˆØ¹
            sharpness: Sharpness adjustment factor | Ù…Ø¹Ø§Ù…Ù„ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø­Ø¯Ø©
            resize: Whether to resize the image | Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³ÙŠØªÙ… ØªØºÙŠÙŠØ± Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø©
            rotate: Angle to rotate the image | Ø²Ø§ÙˆÙŠØ© ØªØ¯ÙˆÙŠØ± Ø§Ù„ØµÙˆØ±Ø©
        Returns:
            torch.Tensor: Preprocessed image tensor | ØªÙ†Ø³ÙˆØ± Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        """
        # Convert to grayscale | Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªØ¯Ø±Ø¬ Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠ
        image = image.convert('L')
        
        # Enhance the image | ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±Ø©
        image = ImageEnhance.Contrast(image).enhance(contrast)
        image = ImageEnhance.Brightness(image).enhance(brightness)
        image = ImageEnhance.Sharpness(image).enhance(sharpness)
        
        # Resize if needed | ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù… Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
        if resize:
            image = image.resize((224, 224))
        
        # Rotate if specified | Ø§Ù„ØªØ¯ÙˆÙŠØ± Ø¥Ø°Ø§ ØªÙ… ØªØ­Ø¯ÙŠØ¯Ù‡
        if rotate:
            image = image.rotate(rotate)
        
        # Convert to tensor | Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙˆØ±
        tensor = torch.from_numpy(np.array(image)).float()
        tensor = tensor.unsqueeze(0).unsqueeze(0)
        tensor = tensor.repeat(1, 3, 1, 1)  # Convert to 3 channels | Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ 3 Ù‚Ù†ÙˆØ§Øª
        tensor = tensor / 255.0  # Normalize to [0, 1] | Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø¥Ù„Ù‰ [0, 1]
        
        # Normalize with ImageNet stats | Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ImageNet
        normalize = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
        tensor = normalize(tensor)
        
        return tensor.to(self.device)
    
    def draw_result_on_image(self, image: Image.Image, boxes, predictions) -> Image.Image:
        """
        Draw bounding box and Arabic letter on the image
        Ø±Ø³Ù… Ø§Ù„Ù…Ø±Ø¨Ø¹ Ø§Ù„Ù…Ø­ÙŠØ· ÙˆØ§Ù„Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©
        """
        # Create a copy of the image | Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©
        result_image = image.copy()
        draw = ImageDraw.Draw(result_image)
        width, height = image.size
        
        for box, prediction in zip(boxes, predictions):
            x, y, w, h = box
            box_left = x
            box_right = x + w
            box_top = y
            box_bottom = y + h
            
            # Draw a thinner green box | Ø±Ø³Ù… Ù…Ø±Ø¨Ø¹ Ø£Ø®Ø¶Ø± Ø±ÙÙŠØ¹
            box_color = (0, 255, 0)  # Green color | Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£Ø®Ø¶Ø±
            box_width = 1  # Thinner line | Ø®Ø· Ø£Ø±ÙØ¹
            draw.rectangle(
                [(box_left, box_top), (box_right, box_bottom)],
                outline=box_color,
                width=box_width
            )
            
            try:
                # Add the Arabic letter with adjusted position | Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…Ø¹ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¶Ø¹
                font_size = min(h, w) // 2  # Adjusted font size | Ø­Ø¬Ù… Ø§Ù„Ø®Ø· Ø§Ù„Ù…Ø¹Ø¯Ù„
                font_path = str(Path(__file__).parent / 'assets' / 'fonts' / 'NotoSansArabic-Regular.ttf')
                if os.path.exists(font_path):
                    font = ImageFont.truetype(font_path, font_size)
                else:
                    font = ImageFont.load_default()
                
                # Calculate text position | Ø­Ø³Ø§Ø¨ Ù…ÙˆØ¶Ø¹ Ø§Ù„Ù†Øµ
                text = self.letter_mapping[prediction]['letter']
                text_bbox = draw.textbbox((0, 0), text, font=font)
                text_width = text_bbox[2] - text_bbox[0]
                text_height = text_bbox[3] - text_bbox[1]
                
                # Center text above the box | ØªÙˆØ³ÙŠØ· Ø§Ù„Ù†Øµ ÙÙˆÙ‚ Ø§Ù„Ù…Ø±Ø¨Ø¹
                text_x = box_left + (w - text_width) // 2
                text_y = box_top - text_height - 5  # Small gap above box | ÙØ±Ø§Øº ØµØºÙŠØ± ÙÙˆÙ‚ Ø§Ù„Ù…Ø±Ø¨Ø¹
                
                # Draw text | Ø±Ø³Ù… Ø§Ù„Ù†Øµ
                draw.text((text_x, text_y), text, fill=(255, 0, 0), font=font)
            except Exception as e:
                logging.error(f"Error drawing text: {str(e)}")
                continue
        
        return result_image 

    def predict(self, image: Image.Image,
                contrast: float = 2.0,
                brightness: float = 1.2,
                sharpness: float = 1.5,
                num_predictions: int = 5,
                image_type: str = 'white_background',
                return_boxes: bool = False):  
        """
        Make prediction on the preprocessed image
        Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        
        Args:
            image: Input image | Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø©
            contrast: Contrast adjustment | ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªØ¨Ø§ÙŠÙ†
            brightness: Brightness adjustment | ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø³Ø·ÙˆØ¹
            sharpness: Sharpness adjustment | ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø­Ø¯Ø©
            num_predictions: Number of predictions to return | Ø¹Ø¯Ø¯ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¥Ø±Ø¬Ø§Ø¹Ù‡Ø§
            image_type: Type of image processing to apply | Ù†ÙˆØ¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ·Ø¨ÙŠÙ‚Ù‡Ø§:
                       'white_background': For clean images with white background | Ù„Ù„ØµÙˆØ± Ø§Ù„Ù†Ø¸ÙŠÙØ© Ø°Ø§Øª Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡
                       'dark_background': For images with dark background | Ù„Ù„ØµÙˆØ± Ø°Ø§Øª Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„Ø¯Ø§ÙƒÙ†Ø©
                       'inscription': For inscription/rock carving images | Ù„ØµÙˆØ± Ø§Ù„Ù†Ù‚ÙˆØ´/Ø§Ù„Ù†Ø­Øª Ø¹Ù„Ù‰ Ø§Ù„ØµØ®ÙˆØ±
            return_boxes: Whether to return the bounding boxes | Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³ÙŠØªÙ… Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª Ø§Ù„Ù…Ø­ÙŠØ·Ø©
        """
        try:
            # Convert to grayscale | Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªØ¯Ø±Ø¬ Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠ
            gray_image = image.convert('L')
            
            if image_type == 'white_background':
                # Process images with white background | Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± Ø°Ø§Øª Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡
                enhanced = ImageEnhance.Contrast(gray_image).enhance(contrast)
                enhanced = ImageEnhance.Brightness(enhanced).enhance(brightness)
                enhanced = ImageEnhance.Sharpness(enhanced).enhance(sharpness)
                
                img_array = np.array(enhanced)
                _, binary = cv2.threshold(img_array, 127, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
                
                kernel = np.ones((2,2), np.uint8)
                binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
                
                # Invert the image to make letters white | Ø¹ÙƒØ³ Ø§Ù„ØµÙˆØ±Ø© Ù„Ø¬Ø¹Ù„ Ø§Ù„Ø­Ø±ÙˆÙ Ø¨ÙŠØ¶Ø§Ø¡
                binary = 255 - binary
                
            elif image_type == 'dark_background':
                # Process images with dark background | Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± Ø°Ø§Øª Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„Ø¯Ø§ÙƒÙ†Ø©
                enhanced = ImageEnhance.Contrast(gray_image).enhance(contrast * 1.5)
                enhanced = ImageEnhance.Brightness(enhanced).enhance(brightness * 1.3)
                
                img_array = np.array(enhanced)
                # Use adaptive thresholding for dark backgrounds | Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹ØªØ¨Ø© Ø§Ù„ØªÙƒÙŠÙ Ù„Ù„Ø®Ù„ÙÙŠØ§Øª Ø§Ù„Ø¯Ø§ÙƒÙ†Ø©
                binary = cv2.adaptiveThreshold(
                    img_array, 255,
                    cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                    cv2.THRESH_BINARY,
                    21, 10
                )
                
                kernel = np.ones((3,3), np.uint8)
                binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
                binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
                
            else:  # image_type == 'inscription'
                # Process inscription images | Ù…Ø¹Ø§Ù„Ø¬Ø© ØµÙˆØ± Ø§Ù„Ù†Ù‚ÙˆØ´
                # Apply local contrast enhancement | ØªØ·Ø¨ÙŠÙ‚ ØªØ¹Ø²ÙŠØ² Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø§Ù„Ù…Ø­Ù„ÙŠ
                clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
                enhanced = clahe.apply(np.array(gray_image))
                
                # Apply a filter to reduce noise | ØªØ·Ø¨ÙŠÙ‚ Ù…Ø±Ø´Ø­ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡
                enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)
                
                # Use adaptive thresholding with custom values for inscriptions | Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹ØªØ¨Ø© Ø§Ù„ØªÙƒÙŠÙ Ù…Ø¹ Ù‚ÙŠÙ… Ù…Ø®ØµØµØ© Ù„Ù„Ù†Ù‚ÙˆØ´
                binary = cv2.adaptiveThreshold(
                    enhanced, 255,
                    cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                    cv2.THRESH_BINARY,
                    25, 15
                )
                
                # Apply morphological operations to clean the image | ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù…Ù„ÙŠØ§Øª Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ© Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØµÙˆØ±Ø©
                kernel = np.ones((3,3), np.uint8)
                binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
                binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
                
                # Apply an additional filter to smooth edges | ØªØ·Ø¨ÙŠÙ‚ Ù…Ø±Ø´Ø­ Ø¥Ø¶Ø§ÙÙŠ Ù„ØªÙ†Ø¹ÙŠÙ… Ø§Ù„Ø­ÙˆØ§Ù
                binary = cv2.medianBlur(binary, 3)
            
            # Find connected components with adjusted connectivity | Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…ØªØµÙ„Ø© Ù…Ø¹ Ø§Ù„ØªÙˆØµÙŠÙ„ Ø§Ù„Ù…Ø¹Ø¯Ù„
            connectivity = 4 if image_type == 'white_background' else 8
            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary, connectivity=connectivity)
            
            boxes = []
            valid_components = []
            
            # Adjust filtering criteria based on image type | ØªØ¹Ø¯ÙŠÙ„ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªØµÙÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„ØµÙˆØ±Ø©
            for i in range(1, num_labels):
                x = stats[i, cv2.CC_STAT_LEFT]
                y = stats[i, cv2.CC_STAT_TOP]
                w = stats[i, cv2.CC_STAT_WIDTH]
                h = stats[i, cv2.CC_STAT_HEIGHT]
                area = stats[i, cv2.CC_STAT_AREA]
                
                # Adjust criteria based on image type | ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„ØµÙˆØ±Ø©
                if image_type == 'white_background':
                    min_area = 50
                    max_area = (image.size[0] * image.size[1]) / 6
                    min_aspect = 0.1
                    max_aspect = 10
                    min_density = 0.05
                    max_density = 0.95
                    margin = 3
                elif image_type == 'dark_background':
                    min_area = 40
                    max_area = (image.size[0] * image.size[1]) / 5
                    min_aspect = 0.08
                    max_aspect = 12
                    min_density = 0.03
                    max_density = 0.97
                    margin = 4
                else:  # inscription
                    min_area = 30
                    max_area = (image.size[0] * image.size[1]) / 4
                    min_aspect = 0.05
                    max_aspect = 15
                    min_density = 0.02
                    max_density = 0.98
                    margin = 5
                
                aspect_ratio = w / h if h > 0 else 0
                
                if (min_area < area < max_area and 
                    min_aspect < aspect_ratio < max_aspect):
                    
                    component_mask = (labels == i).astype(np.uint8)
                    black_pixels = np.sum(component_mask)
                    density = black_pixels / (w * h)
                    
                    if min_density < density < max_density:
                        x = max(0, x - margin)
                        y = max(0, y - margin)
                        w = min(image.size[0] - x, w + 2 * margin)
                        h = min(image.size[1] - y, h + 2 * margin)
                        boxes.append((x, y, w, h))
                        valid_components.append(i)
            
            # Sort boxes from right to left | ÙØ±Ø² Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª Ù…Ù† Ø§Ù„ÙŠÙ…ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø±
            boxes.sort(key=lambda box: -(box[0] + box[2]))
            
            # Prepare letter images for prediction | Ø¥Ø¹Ø¯Ø§Ø¯ ØµÙˆØ± Ø§Ù„Ø­Ø±ÙˆÙ Ù„Ù„ØªÙ†Ø¨Ø¤
            letter_images = []
            final_boxes = []
            for box in boxes:
                x, y, w, h = box
                letter_image = image.crop((x, y, x+w, y+h))
                letter_images.append(letter_image)
                final_boxes.append(box)
            
            # Adjust confidence threshold based on image type | ØªØ¹Ø¯ÙŠÙ„ Ø¹ØªØ¨Ø© Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„ØµÙˆØ±Ø©
            confidence_threshold = {
                'white_background': 0.4,
                'dark_background': 0.35,
                'inscription': 0.3
            }.get(image_type, 0.4)
            
            # Predict letters | Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø­Ø±ÙˆÙ
            predictions = []
            confidences = []
            for letter_image in letter_images:
                processed_image = self.preprocess_image(letter_image)
                
                with torch.no_grad():
                    outputs = self.model(processed_image)
                    probabilities = torch.softmax(outputs, dim=1)
                    confidence, prediction = torch.max(probabilities, 1)
                    
                    if confidence.item() > confidence_threshold:
                        predictions.append(prediction.item())
                        confidences.append(confidence.item())
                    else:
                        predictions.append(-1)
                        confidences.append(0.0)
            
            # Filter out low-confidence predictions | ØªØµÙÙŠØ© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø°Ø§Øª Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø©
            final_predictions = []
            final_confidences = []
            filtered_boxes = []
            for pred, conf, box in zip(predictions, confidences, final_boxes):
                if pred != -1:
                    final_predictions.append(pred)
                    final_confidences.append(conf)
                    filtered_boxes.append(box)
            
            # Draw result on image | Ø±Ø³Ù… Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©
            result_image = self.draw_result_on_image(image, filtered_boxes, final_predictions)
            
            if return_boxes:
                return final_predictions, final_confidences, result_image, filtered_boxes
            else:
                return final_predictions, final_confidences, result_image
            
        except Exception as e:
            logging.error(f"Error making prediction: {str(e)}")
            raise

def setup_page():
    """
    Setup the application page
    Ø¥Ø¹Ø¯Ø§Ø¯ ØµÙØ­Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
    """
    st.set_page_config(
        page_title="Thamudic Character Recognition",
        page_icon="ğŸ”",
        layout="wide"
    )
    
    # CSS for design | CSS Ù„Ù„ØªØµÙ…ÙŠÙ…
    st.markdown("""
    <style>
        .element-container, .stMarkdown, .stButton, .stText {
            text-align: center;
        }
        .stImage > img {
            max-width: 400px;
            margin: auto;
        }
        .prediction-box {
            padding: 20px;
            border-radius: 10px;
            background-color: #1e3d59;
            color: white;
            margin: 10px 0;
            border: 2px solid #ffc13b;
        }
    </style>
    """, unsafe_allow_html=True)

def main():
    """
    Main application function
    Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    """
    setup_page()
    st.title("Thamudic Character Recognition ğŸ”")
    st.markdown("---")
    
    try:
        # Initialize paths | ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
        base_dir = Path(__file__).parent.parent
        model_path = base_dir / 'models' / 'checkpoints' / 'best_model.pth'
        mapping_path = base_dir / 'data' / 'letter_mapping.json'
        
        # Initialize the application | ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
        app = ThamudicApp(str(model_path), str(mapping_path))
        st.success("Model loaded successfully!")
        
        # Create tabs for different functionalities | Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ¨ÙˆÙŠØ¨ Ù„Ù…Ø®ØªÙ„Ù Ø§Ù„ÙˆØ¸Ø§Ø¦Ù
        tabs = st.tabs(["Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚ÙˆØ´", "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Ù‚ÙˆØ´"])
        
        with tabs[0]:  # Recognition tab | Ø¹Ù„Ø§Ù…Ø© ØªØ¨ÙˆÙŠØ¨ Ø§Ù„ØªØ¹Ø±Ù
            # Create two columns: left for settings, right for results | Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù…ÙˆØ¯ÙŠÙ†: Ø§Ù„ÙŠØ³Ø§Ø± Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§ØªØŒ Ø§Ù„ÙŠÙ…ÙŠÙ† Ù„Ù„Ù†ØªØ§Ø¦Ø¬
            col_settings, col_results = st.columns([1, 2])
            
            with col_settings:
                st.subheader("âš™ï¸ Recognition Settings")
                # Ø¥Ø¶Ø§ÙØ© Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„ØµÙˆØ±Ø©
                image_type = st.selectbox(
                    "Ù†ÙˆØ¹ Ø§Ù„ØµÙˆØ±Ø©",
                    options=['white_background', 'dark_background', 'inscription'],
                    format_func=lambda x: {
                        'white_background': 'Ø®Ù„ÙÙŠØ© Ø¨ÙŠØ¶Ø§Ø¡ (ØµÙˆØ± Ù†Ø¸ÙŠÙØ©)',
                        'dark_background': 'Ø®Ù„ÙÙŠØ© Ø³ÙˆØ¯Ø§Ø¡',
                        'inscription': 'Ù†Ù‚ÙˆØ´ ØµØ®Ø±ÙŠØ©'
                    }[x]
                )
                
                # Image processing settings | Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©
                contrast = st.slider("Contrast", 0.5, 3.0, 2.0, 0.1)
                brightness = st.slider("Brightness", 0.5, 3.0, 1.2, 0.1)
                sharpness = st.slider("Sharpness", 0.5, 3.0, 1.5, 0.1)
            
            with col_results:
                st.subheader("Recognition Results")
                # File uploader | Ù…Ø±ÙÙˆØ¹ Ù…Ù„Ù
                uploaded_file = st.file_uploader("Choose an image...", type=['png', 'jpg', 'jpeg'], key="recognition_uploader")
                
                # Create placeholder for image and results | Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙƒØ§Ù† Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„ØµÙˆØ±Ø© ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬
                image_placeholder = st.empty()
                letters_container = st.container()
                
                if uploaded_file is not None:
                    # Read the image | Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø©
                    image = Image.open(uploaded_file)
                    
                    # Process image automatically | Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§
                    with st.spinner('Processing...'):
                        # Make prediction | Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤
                        predictions, confidences, result_image = app.predict(
                            image,
                            contrast=contrast,
                            brightness=brightness,
                            sharpness=sharpness,
                            image_type=image_type
                        )
                        
                        # Display result image | Ø¹Ø±Ø¶ ØµÙˆØ±Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
                        image_placeholder.image(result_image, use_container_width=True)
                        
                        # Display detected letters | Ø¹Ø±Ø¶ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…ÙƒØªØ´ÙØ©
                        if predictions:
                            with letters_container:
                                st.markdown("### Detected Letters")
                                # Use columns to display letters in a grid | Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø­Ø±ÙˆÙ ÙÙŠ Ø´Ø¨ÙƒØ©
                                letter_cols = st.columns(3)
                                for idx, (pred, conf) in enumerate(zip(predictions, confidences)):
                                    letter_info = app.letter_mapping[pred]
                                    with letter_cols[idx % 3]:
                                        st.markdown(
                                            f"""
                                            <div style="
                                                padding: 10px;
                                                border-radius: 5px;
                                                background-color: rgba(30, 61, 89, 0.9);
                                                color: white;
                                                margin: 5px 0;
                                                text-align: center;
                                                border: 1px solid #ffc13b;">
                                                <h4 style="margin:0;font-size:1.5em;">{letter_info['letter']}</h4>
                                                <p style="margin:0;font-size:0.9em;">({letter_info['symbol']})</p>
                                                <p style="margin:0;font-size:0.8em;color:#ffc13b;">
                                                    {conf:.1%}
                                                </p>
                                            </div>
                                            """,
                                            unsafe_allow_html=True
                                        )
                        else:
                            with letters_container:
                                st.warning("No letters detected in the image.")
        
        with tabs[1]:  # Background removal tab | Ø¹Ù„Ø§Ù…Ø© ØªØ¨ÙˆÙŠØ¨ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø®Ù„ÙÙŠØ©
            st.subheader("ğŸ–¼ï¸ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Ù‚ÙˆØ´ - Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø®Ù„ÙÙŠØ©")
            st.markdown("Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø© Ù„ØªØ­ÙˆÙŠÙ„ ØµÙˆØ± Ø§Ù„Ù†Ù‚ÙˆØ´ Ø§Ù„ØµØ®Ø±ÙŠØ© Ø¥Ù„Ù‰ Ù†Ù‚ÙˆØ´ Ø¨Ø¯ÙˆÙ† Ø®Ù„ÙÙŠØ©")
            
            # Create two columns for settings and results | Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù…ÙˆØ¯ÙŠÙ† Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆÙ†ØªØ§Ø¦Ø¬
            col_settings_bg, col_results_bg = st.columns([1, 2])
            
            with col_settings_bg:
                # Background removal settings | Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø®Ù„ÙÙŠØ©
                st.markdown("### Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
                
                # Add sliders for background removal parameters | Ø¥Ø¶Ø§ÙØ© Ø´Ø±ÙŠØ· Ø²Ù„Ù‚ Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø®Ù„ÙÙŠØ©
                threshold_method = st.selectbox(
                    "Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©",
                    options=['adaptive', 'otsu', 'binary'],
                    format_func=lambda x: {
                        'adaptive': 'ØªÙƒÙŠÙÙŠØ© (Ù„Ù„Ù†Ù‚ÙˆØ´ ØºÙŠØ± Ø§Ù„ÙˆØ§Ø¶Ø­Ø©)',
                        'otsu': 'Ø£ÙˆØªØ³Ùˆ (Ù„Ù„Ù†Ù‚ÙˆØ´ Ø§Ù„ÙˆØ§Ø¶Ø­Ø©)',
                        'binary': 'Ø«Ù†Ø§Ø¦ÙŠØ© (Ù„Ù„Ù†Ù‚ÙˆØ´ Ø¹Ø§Ù„ÙŠØ© Ø§Ù„ØªØ¨Ø§ÙŠÙ†)'
                    }[x]
                )
                
                threshold_value = st.slider("Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹ØªØ¨Ø©", 0, 255, 127, 1)
                block_size = st.slider("Ø­Ø¬Ù… Ø§Ù„ÙƒØªÙ„Ø© (Ù„Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙƒÙŠÙÙŠØ©)", 3, 99, 11, 2)
                c_value = st.slider("Ù‚ÙŠÙ…Ø© C (Ù„Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙƒÙŠÙÙŠØ©)", -10, 30, 2, 1)
                
                # Morphological operations | Ø¹Ù…Ù„ÙŠØ§Øª Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ©
                kernel_size = st.slider("Ø­Ø¬Ù… Ø§Ù„Ù†ÙˆØ§Ø©", 1, 9, 3, 2)
                
                # Post-processing options | Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„Ø§Ø­Ù‚Ø©
                apply_blur = st.checkbox("ØªØ·Ø¨ÙŠÙ‚ ØªÙ†Ø¹ÙŠÙ…", value=True)
                blur_amount = st.slider("Ù…Ù‚Ø¯Ø§Ø± Ø§Ù„ØªÙ†Ø¹ÙŠÙ…", 1, 15, 3, 2) if apply_blur else 3
                
                invert_output = st.checkbox("Ø¹ÙƒØ³ Ø§Ù„Ø£Ù„ÙˆØ§Ù†", value=True)
                
                # Color options | Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø£Ù„ÙˆØ§Ù†
                color_mode = st.selectbox(
                    "Ù†Ù…Ø· Ø§Ù„Ø£Ù„ÙˆØ§Ù†",
                    options=['black_white', 'custom'],
                    format_func=lambda x: {
                        'black_white': 'Ø£Ø¨ÙŠØ¶ ÙˆØ£Ø³ÙˆØ¯',
                        'custom': 'Ù„ÙˆÙ† Ù…Ø®ØµØµ'
                    }[x]
                )
                
                if color_mode == 'custom':
                    inscription_color = st.color_picker("Ù„ÙˆÙ† Ø§Ù„Ù†Ù‚Ø´", "#FFFFFF")
                    background_color = st.color_picker("Ù„ÙˆÙ† Ø§Ù„Ø®Ù„ÙÙŠØ©", "#000000")
                
                # Add option to recognize characters on the processed image | Ø¥Ø¶Ø§ÙØ© Ø®ÙŠØ§Ø± Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­Ø±Ù ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
                recognize_chars = st.checkbox("Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­Ø±Ù ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©", value=True)
                
                # Recognition settings (only show if recognition is enabled) | Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¹Ø±Ù (ØªØ¸Ù‡Ø± ÙÙ‚Ø· Ø¥Ø°Ø§ ØªÙ… ØªÙ…ÙƒÙŠÙ† Ø§Ù„ØªØ¹Ø±Ù)
                if recognize_chars:
                    st.markdown("### Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­Ø±Ù")
                    recognition_contrast = st.slider("Contrast Ù„Ù„ØªØ¹Ø±Ù", 0.5, 3.0, 2.0, 0.1)
                    recognition_brightness = st.slider("Brightness Ù„Ù„ØªØ¹Ø±Ù", 0.5, 3.0, 1.2, 0.1)
                    recognition_sharpness = st.slider("Sharpness Ù„Ù„ØªØ¹Ø±Ù", 0.5, 3.0, 1.5, 0.1)
            
            with col_results_bg:
                st.markdown("### Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
                # File uploader for background removal | Ù…Ø±ÙÙˆØ¹ Ù…Ù„Ù Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø®Ù„ÙÙŠØ©
                uploaded_file_bg = st.file_uploader("Ø§Ø®ØªØ± ØµÙˆØ±Ø© Ø§Ù„Ù†Ù‚Ø´...", type=['png', 'jpg', 'jpeg'], key="bg_removal_uploader")
                
                # Create placeholders for original and processed images | Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙƒØ§Ù† Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØ§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
                col_orig, col_proc = st.columns(2)
                
                with col_orig:
                    st.markdown("#### Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©")
                    orig_placeholder = st.empty()
                
                with col_proc:
                    st.markdown("#### Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
                    proc_placeholder = st.empty()
                
                # Download button placeholder | Ù…ÙƒØ§Ù† Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ø²Ø± Ø§Ù„ØªÙ†Ø²ÙŠÙ„
                download_placeholder = st.empty()
                
                # Character recognition results placeholder | Ù…ÙƒØ§Ù† Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­Ø±Ù
                recognition_placeholder = st.empty()
                
                if uploaded_file_bg is not None:
                    # Read the image | Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø©
                    image = Image.open(uploaded_file_bg)
                    
                    # Display original image | Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
                    orig_placeholder.image(image, use_container_width=True)
                    
                    # Process image to remove background | Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø®Ù„ÙÙŠØ©
                    with st.spinner('Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...'):
                        # Convert to grayscale | Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªØ¯Ø±Ø¬ Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠ
                        gray_image = image.convert('L')
                        img_array = np.array(gray_image)
                        
                        # Apply threshold based on selected method | ØªØ·Ø¨ÙŠÙ‚ Ø¹ØªØ¨Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
                        if threshold_method == 'adaptive':
                            # Ensure block_size is odd | Ø¶Ù…Ø§Ù† Ø£Ù† ÙŠÙƒÙˆÙ† Ø­Ø¬Ù… Ø§Ù„ÙƒØªÙ„Ø© ÙØ±Ø¯ÙŠÙ‹Ø§
                            if block_size % 2 == 0:
                                block_size += 1
                                
                            binary = cv2.adaptiveThreshold(
                                img_array, 255,
                                cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                cv2.THRESH_BINARY,
                                block_size, c_value
                            )
                        elif threshold_method == 'otsu':
                            _, binary = cv2.threshold(img_array, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
                        else:  # binary
                            _, binary = cv2.threshold(img_array, threshold_value, 255, cv2.THRESH_BINARY)
                        
                        # Apply morphological operations | ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù…Ù„ÙŠØ§Øª Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ©
                        kernel = np.ones((kernel_size, kernel_size), np.uint8)
                        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
                        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
                        
                        # Apply blur if selected | ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†Ø¹ÙŠÙ… Ø¥Ø°Ø§ ØªÙ… ØªØ­Ø¯ÙŠØ¯Ù‡
                        if apply_blur:
                            binary = cv2.medianBlur(binary, blur_amount)
                        
                        # Invert if needed | Ø¹ÙƒØ³ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                        if invert_output:
                            binary = 255 - binary
                        
                        # Apply custom colors if selected | ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ù…Ø®ØµØµØ© Ø¥Ø°Ø§ ØªÙ… ØªØ­Ø¯ÙŠØ¯Ù‡Ø§
                        if color_mode == 'custom':
                            # Convert hex colors to RGB | ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø³Ø¯Ø§Ø³ÙŠØ© Ø¥Ù„Ù‰ RGB
                            def hex_to_rgb(hex_color):
                                hex_color = hex_color.lstrip('#')
                                return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))
                            
                            insc_color = hex_to_rgb(inscription_color)
                            bg_color = hex_to_rgb(background_color)
                            
                            # Create colored image | Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ù…Ù„ÙˆÙ†Ø©
                            colored_image = np.zeros((binary.shape[0], binary.shape[1], 3), dtype=np.uint8)
                            
                            # Set colors based on binary mask | ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‚Ù†Ø§Ø¹ Ø«Ù†Ø§Ø¦ÙŠ
                            if invert_output:
                                colored_image[binary == 0] = insc_color
                                colored_image[binary == 255] = bg_color
                            else:
                                colored_image[binary == 255] = insc_color
                                colored_image[binary == 0] = bg_color
                                
                            result_image = Image.fromarray(colored_image)
                        else:
                            # Use black and white | Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³ÙˆØ¯ ÙˆØ§Ù„Ø£Ø¨ÙŠØ¶
                            result_image = Image.fromarray(binary)
                        
                        # Recognize characters if enabled | Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­Ø±Ù Ø¥Ø°Ø§ ØªÙ… ØªÙ…ÙƒÙŠÙ†Ù‡Ø§
                        if recognize_chars:
                            # Convert processed image to RGB for character recognition | ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¥Ù„Ù‰ RGB Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­Ø±Ù
                            if result_image.mode != 'RGB':
                                rgb_result = result_image.convert('RGB')
                            else:
                                rgb_result = result_image
                            
                            # Make prediction using the processed image | Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
                            with st.spinner('Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­Ø±Ù...'):
                                predictions, confidences, _, boxes_info = app.predict(
                                    rgb_result,
                                    contrast=recognition_contrast,
                                    brightness=recognition_brightness,
                                    sharpness=recognition_sharpness,
                                    image_type='inscription',  # Use inscription mode for processed images | Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¶Ø¹ Ø§Ù„Ù†Ù‚Ø´ Ù„Ù„ØµÙˆØ± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
                                    return_boxes=True
                                )
                                
                                # Create a custom annotated image with red text | Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ù…Ø®ØµØµØ© Ù…Ø¹ Ù†Øµ Ø£Ø­Ù…Ø±
                                if predictions:
                                    # Create a copy of the result image for annotation | Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ù…Ù† ØµÙˆØ±Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ‚
                                    annotated_image = rgb_result.copy()
                                    draw = ImageDraw.Draw(annotated_image)
                                    
                                    # Draw boxes and text in red | Ø±Ø³Ù… Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª ÙˆØ§Ù„Ù†Øµ Ø§Ù„Ø£Ø­Ù…Ø±
                                    for box, prediction in zip(boxes_info, predictions):
                                        x, y, w, h = box
                                        box_left = x
                                        box_right = x + w
                                        box_top = y
                                        box_bottom = y + h
                                        
                                        # Draw a red box | Ø±Ø³Ù… Ù…Ø±Ø¨Ø¹ Ø£Ø­Ù…Ø±
                                        box_color = (255, 0, 0)  # Red color | Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£Ø­Ù…Ø±
                                        box_width = 2  # Slightly thicker line | Ø®Ø· Ø£Ø±ÙØ¹ Ù‚Ù„ÙŠÙ„Ø§Ù‹
                                        draw.rectangle(
                                            [(box_left, box_top), (box_right, box_bottom)],
                                            outline=box_color,
                                            width=box_width
                                        )
                                        
                                        try:
                                            # Add the Arabic letter with adjusted position | Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…Ø¹ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¶Ø¹
                                            font_size = min(h, w) // 2  # Adjusted font size | Ø­Ø¬Ù… Ø§Ù„Ø®Ø· Ø§Ù„Ù…Ø¹Ø¯Ù„
                                            font_path = str(Path(__file__).parent / 'assets' / 'fonts' / 'NotoSansArabic-Regular.ttf')
                                            if os.path.exists(font_path):
                                                font = ImageFont.truetype(font_path, font_size)
                                            else:
                                                font = ImageFont.load_default()
                                            
                                            # Calculate text position | Ø­Ø³Ø§Ø¨ Ù…ÙˆØ¶Ø¹ Ø§Ù„Ù†Øµ
                                            text = app.letter_mapping[prediction]['letter']
                                            text_bbox = draw.textbbox((0, 0), text, font=font)
                                            text_width = text_bbox[2] - text_bbox[0]
                                            text_height = text_bbox[3] - text_bbox[1]
                                            
                                            # Center text above the box | ØªÙˆØ³ÙŠØ· Ø§Ù„Ù†Øµ ÙÙˆÙ‚ Ø§Ù„Ù…Ø±Ø¨Ø¹
                                            text_x = box_left + (w - text_width) // 2
                                            text_y = box_top - text_height - 5  # Small gap above box | ÙØ±Ø§Øº ØµØºÙŠØ± ÙÙˆÙ‚ Ø§Ù„Ù…Ø±Ø¨Ø¹
                                            
                                            # Draw text in red | Ø±Ø³Ù… Ø§Ù„Ù†Øµ Ø§Ù„Ø£Ø­Ù…Ø±
                                            draw.text((text_x, text_y), text, fill=(255, 0, 0), font=font)
                                        except Exception as e:
                                            logging.error(f"Error drawing text: {str(e)}")
                                            continue
                                else:
                                    # If no predictions, just use the processed image | Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ ØªÙ†Ø¨Ø¤Ø§ØªØŒ Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
                                    annotated_image = rgb_result
                                
                                # Display the annotated image | Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ù„Ù‚Ø©
                                proc_placeholder.image(annotated_image, use_container_width=True)
                                
                                # Display detected letters | Ø¹Ø±Ø¶ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…ÙƒØªØ´ÙØ©
                                if predictions:
                                    with recognition_placeholder.container():
                                        st.markdown("### Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ù…ØªØ¹Ø±Ù Ø¹Ù„ÙŠÙ‡Ø§")
                                        # Use columns to display letters in a grid | Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø­Ø±ÙˆÙ ÙÙŠ Ø´Ø¨ÙƒØ©
                                        letter_cols = st.columns(3)
                                        for idx, (pred, conf) in enumerate(zip(predictions, confidences)):
                                            letter_info = app.letter_mapping[pred]
                                            with letter_cols[idx % 3]:
                                                st.markdown(
                                                    f"""
                                                    <div style="
                                                        padding: 10px;
                                                        border-radius: 5px;
                                                        background-color: rgba(30, 61, 89, 0.9);
                                                        color: white;
                                                        margin: 5px 0;
                                                        text-align: center;
                                                        border: 1px solid #ffc13b;">
                                                        <h4 style="margin:0;font-size:1.5em;">{letter_info['letter']}</h4>
                                                        <p style="margin:0;font-size:0.9em;">({letter_info['symbol']})</p>
                                                        <p style="margin:0;font-size:0.8em;color:#ffc13b;">
                                                            {conf:.1%}
                                                        </p>
                                                    </div>
                                                    """,
                                                    unsafe_allow_html=True
                                                )
                                else:
                                    recognition_placeholder.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø£ÙŠ Ø£Ø­Ø±Ù ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©.")
                                
                                # Create download button for the annotated image | Ø¥Ù†Ø´Ø§Ø¡ Ø²Ø± ØªÙ†Ø²ÙŠÙ„ Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ù„Ù‚Ø©
                                buf = io.BytesIO()
                                annotated_image.save(buf, format="PNG")
                                byte_im = buf.getvalue()
                        else:
                            # Display the processed image without annotations | Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø¯ÙˆÙ† ØªØ¹Ù„ÙŠÙ‚Ø§Øª
                            proc_placeholder.image(result_image, use_container_width=True)
                            
                            # Create download button for the processed image | Ø¥Ù†Ø´Ø§Ø¡ Ø²Ø± ØªÙ†Ø²ÙŠÙ„ Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
                            buf = io.BytesIO()
                            result_image.save(buf, format="PNG")
                            byte_im = buf.getvalue()
                        
                        # Update download button | ØªØ­Ø¯ÙŠØ« Ø²Ø± Ø§Ù„ØªÙ†Ø²ÙŠÙ„
                        download_placeholder.download_button(
                            label="ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©",
                            data=byte_im,
                            file_name="processed_inscription.png",
                            mime="image/png"
                        )
    
    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        logging.error(f"Error in application: {str(e)}")

if __name__ == '__main__':
    main()
